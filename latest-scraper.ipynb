{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-01T12:07:17.576328Z",
     "start_time": "2020-11-01T12:07:07.097428Z"
    }
   },
   "outputs": [],
   "source": [
    "import tweepy as tw\n",
    "import datetime as dt\n",
    "import pandas as pd\n",
    "from keras.models import load_model\n",
    "import numpy as np\n",
    "from pythainlp import word_tokenize\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-01T12:07:17.581271Z",
     "start_time": "2020-11-01T12:07:17.577295Z"
    }
   },
   "outputs": [],
   "source": [
    "# pip install tweepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-01T12:07:17.590260Z",
     "start_time": "2020-11-01T12:07:17.584265Z"
    }
   },
   "outputs": [],
   "source": [
    "def complete_func_v2(input_from_user):\n",
    "    \n",
    "    \n",
    "    # access token key and access secret key => Suphachai Sriwised  ใช้ของใครก้ได้ เอามายืนยันตัวตนให้ api ทำงานได้เฉยๆ\n",
    "    access_token = '247327218-2Y9OmrkjTGBtyKv91zJl7ljwPeEUhkDJIC2lxMnB'\n",
    "    access_secret_key = 'Ygy2CPjPaOY3P88UFbmazzNzRU60CviTsJe99VPvlWnBF'\n",
    "    \n",
    "    \n",
    "\n",
    "    # อันนี้ต้องใช้ของคนที่ได้รับสิทธิ twitter api developper เท่านั้น ถึงจะทำงานได้\n",
    "    consumer_key = 'V6WhIk4MThLDHOZffp5ePjhxb'\n",
    "    consumer_secret_key = '0mBgQEhDmlCJYFiEuzv49bwauCKzvBHh7yg2ph5QksMNxHTaOQ'\n",
    "    \n",
    "    \n",
    "    \n",
    "    # ยืนยันตัวตนเพื่อเข้าใช้งาน twitter API    \n",
    "    auth = tw.OAuthHandler(consumer_key, consumer_secret_key)\n",
    "    auth.set_access_token(access_token, access_secret_key)\n",
    "    api = tw.API(auth, wait_on_rate_limit=True)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # ใช้ method ของ library tweepy เพื่อค้นหาตามคีย์เวิร์ด ซึ่งสามารถกรอกคำสั่งตามที่มีใน doc ของ twitter API ได้เพื่อตัด หรือ ค้น ในสิ่งที่ต้องการ \n",
    "    # คำสั่งจะใช้ตามนี้ https://developer.twitter.com/en/docs/twitter-api/v1/rules-and-filtering/overview/standard-operators    \n",
    "    tweets = tw.Cursor(api.search,\n",
    "              q=input_from_user+' -filter:retweets',\n",
    "              lang=\"th\",\n",
    "              since=\"\").items(500)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # ทำให้เป็น csv (แบบยังไม่มีผลการทำนาย)     \n",
    "    step1 = to_csv_v2(tweets)\n",
    "    print(step1)\n",
    "    \n",
    "    \n",
    "    \n",
    "# ข้ามการ normalize ไปก่อนเพราะเด่วเราจะไปทำที่ auto label ถ้าทำจากอันนั้นจะได้ ออกมาเป็น word,toxic พร้อมใช้งาน \n",
    "# แต่อันนี้ต้องให้คนตรวจอีกรอบก่อน เพราะมันมีสามช่อง  \n",
    "    #ทำการประกาศตัวแปรเป็นโกลบอลเพื่อให้ normaliz_v1 (จะมีสองเวอร์ v1 จะเป็นแบบลูป v2 จะเป็นแบบที่ละอัน) เรียกใช้งานได้ (ความจริงจะประกาศใน func นั้นก้ได้แต่ ขก.ล้วนๆ)\n",
    "#     global clawing_model , dict_fileWord , nonePredict_documents\n",
    "#     clawing_model = load_model('cnn&rnn_model.h5')\n",
    "#     dict_fileWord=pd.read_csv('dict_file.csv')\n",
    "#     nonePredict_documents = pd.read_csv ('dataset_tweet.csv')\n",
    "#     print('this is none predict document csv file : ',test_documents)\n",
    "    \n",
    "\n",
    "#     # normalize จะแปลงข้อมูลให้อยู่ในรูปแบบพร้อมใช้งานสำหรับเทรนโมเดล  \n",
    "#     step2 = normalize_v1(nonePredict_documents)\n",
    "#     print(step2)\n",
    "    \n",
    "    \n",
    "    \n",
    "#     completePredict_file = pd.read_csv ('./new_csv.csv')\n",
    "#     # complete_file\n",
    "# #     print(completePredict_file)\n",
    "    \n",
    "    \n",
    "    \n",
    "    return 'complete step to csv an'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         for tweet in tweets:\n",
    "#             print(tweet.text)\n",
    "#     data_tw  = [[tweet.text] for tweet in tweets] \n",
    "\n",
    "#         users_locs = [[tweet.user.screen_name, tweet.user.location,tweet.text] for tweet in tweets]\n",
    "#         users_locs\n",
    "        \n",
    "#     tweet_text = pd.DataFrame(data=data_tw, \n",
    "#                         columns=[\"tweet\"])\n",
    "#     tweet_text.to_csv('dataset_beta_1.0.csv',index=False,encoding='utf-8-sig')\n",
    "#     # ทำการค้นหาคีย์เวิร์ด    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-01T12:07:17.600221Z",
     "start_time": "2020-11-01T12:07:17.592243Z"
    }
   },
   "outputs": [],
   "source": [
    "# อันใหม่ tweepy \n",
    "# แบบสามแถว\n",
    "def to_csv_v2(tw):    \n",
    "        \n",
    "        # ทำการอ่านไฟล์เก่า เพื่อที่จะเอาข้อมูลจากไฟล์เก่ามาเก็บในลิสต์\n",
    "        old_csv = pd.read_csv('dataset_from_scrapper/dataset_tweet.csv');\n",
    "        \n",
    "        #ถ้ามีในไฟล์เก่าก่อนหน้า ก้จะวนลูปเพื่อเอาค่าเก่ามาด้วย มันจะได้ไม่ทับอันเก่า\n",
    "        if(len(old_csv ) !=0 ):\n",
    "            #สร้างลิสต์เปล่ามารองรับข้อมูลจากไฟล์เก่าที่เราดึงมา\n",
    "            list_tw = []\n",
    "            \n",
    "            for word in old_csv['word']:\n",
    "                # RegEx                  \n",
    "                result_1 = re.sub(r\"http\\S+\", \"\",word)\n",
    "                # ตัว r จะทำให้พวก /n และอื่นๆไม่ทำงาน                \n",
    "                result_2 = re.sub(r\"@\\w+\",\"\",result_1)\n",
    "                result_3 = re.sub(r\"#\\w+\",\"\",result_2)\n",
    "                result_4 = re.sub(r\"\\a\",\"\",result_3)\n",
    "                list_tw.append(result_4)\n",
    "        #print อันนี้เอาไว้ debug เฉยๆ\n",
    "        #print('get old list_tw:',list_tw) \n",
    "        \n",
    "        #ถ้าไม่มีจะสร้างลิสต์เปล่าๆเพื่อเตรียมนำ tweet ที่พึ่งได้มาใส่     \n",
    "        else:\n",
    "            #ถ้าเข้าเคสนี้แสดงว่าไฟล์ก่อนหน้ามันว่างเปล่า\n",
    "            list_tw = []\n",
    "            \n",
    "        #จาก data ที่เราได้จากการใช้ tweepy จะออกมาเป็นในรูปแบบของ object เราจะวนลูปเพื่อดึงข้อมูลที่เป็น text ออกมา หรือก็คือ tweetที่หาเจอ \n",
    "        for t in tw:\n",
    "            result_1 = re.sub(r\"http\\S+\", \"\",t.text)\n",
    "            \n",
    "            result_2 = re.sub(r\"@\\w+\",\"\",result_1)\n",
    "            result_3 = re.sub(r\"#\\w+\",\"\",result_2)\n",
    "            result_4 = re.sub(r\"\\a\",\"\",result_3)\n",
    "            #วนลูปเก็บเข้าไปที่ลิสต์ที่เราสร้างมารองรับก่อนหน้า\n",
    "            list_tw.append(result_4);\n",
    "        \n",
    "        #print อันนี้เอาไว้ debug เฉยๆ\n",
    "        #print('list_tw:',list_tw)\n",
    "        \n",
    "        #ทำการสร้าง dataframe ขึ้นมา ตามรูปแบบที่เราต้องการคือ word,toxic1,toxic2,toxic3 \n",
    "        df = pd.DataFrame({'word':list_tw,\n",
    "                            'toxic1':None,\n",
    "                           'toxic2':None,\n",
    "                           'toxic3':None})\n",
    "        \n",
    "        #ต่อมานำ dataframe ที่สร้างมาจัดเก็บเป็น csv เพื่อนำไปใช้งานในส่วนงานอื่นๆ\n",
    "        df.to_csv('dataset_from_scrapper/dataset_tweet.csv',index=False,encoding='utf-8-sig')\n",
    "        return len(pd.read_csv('dataset_from_scrapper/dataset_tweet.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# เตรียมข้อมูลให้มีรูปแบบที่ใช้ได้ต่อ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-11-01T12:07:13.923Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app \"__main__\" (lazy loading)\n",
      " * Environment: production\n",
      "   WARNING: This is a development server. Do not use it in a production deployment.\n",
      "   Use a production WSGI server instead.\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)\n",
      "127.0.0.1 - - [01/Nov/2020 19:09:10] \"\u001b[37mGET / HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [01/Nov/2020 19:09:10] \"\u001b[33mGET /favicon.ico HTTP/1.1\u001b[0m\" 404 -\n"
     ]
    }
   ],
   "source": [
    "from flask import Flask,render_template,request,url_for\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "\n",
    "@app.route('/')\n",
    "def index():\n",
    "    return render_template('/main.html')\n",
    "\n",
    "@app.route('/save',methods=['GET','POST'])\n",
    "def save():\n",
    "\n",
    "    #รับ req เป็นแบบ post เข้ามา\n",
    "    if request.method == 'POST':\n",
    "\n",
    "        #นำคำที่ user ใส่เข้ามาเก็บเอาไว่ที่ keyword_search\n",
    "        keyword_search = request.form['keyword']\n",
    "\n",
    "        # print keyword ออกมาดูเพื่อ debug    \n",
    "        print('this is keyword =>>> ', keyword_search)\n",
    "        \n",
    "        #ทำการเรียก function นี้จากนั้น function นี้จะทำการ call function อื่นๆเพื่อทำงานต่อไป\n",
    "        step3 = complete_func_v2(keyword_search)\n",
    "        print(step3)\n",
    "        \n",
    "        #ทำการแจ้งเตือนผ่าน alert.html เมื่อทำงานเสร็จสิ้นแล้ว\n",
    "        return render_template('alert.html')\n",
    "    else:\n",
    "        #แจ้ง error เมื่อส่ง req แบบอื่นๆเข้ามา\n",
    "        return 'error 400 please use post method'\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>toxic1</th>\n",
       "      <th>toxic2</th>\n",
       "      <th>toxic3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>หวั่นระบาดซ้ำ! ฮ่องกง ขยายเวลาเว้นระยะห่างป้อง...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Suzu Hirose นักแสดงชาวญี่ปุ่นได้รับการยืนยันว่...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ัดพระธรรมกาย&amp;amp;มูลนิธิธรรมกาย\\nมอบความช่วยเ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ตะลึง! องค์การอนามัยโลก (WHO) ชี้ผู้ติดเชื้อ ิ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>อย่าลืมสวมหน้ากาก รักษาระยะห่าง หมั่นล้างมือ น...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6851</th>\n",
       "      <td>ช่วยกันค่ะ  ๊อป18ตุลา    ๊อป14ตุลา</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6852</th>\n",
       "      <td>เกลียดเผด็จการ ีดเส้นใต้ไล่เผด็จการ ็อบ18ตุลา</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6853</th>\n",
       "      <td>ทำไงดีวะกูอยู่ที่บ้านสลิ่มนั่งฟังจนป่วยจิตหมดล...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6854</th>\n",
       "      <td>\"ไม่รักพ่อ ออกจากบ้านพ่อไป\" อ๋อ ออกไม่ได้ค่ะ ช...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6855</th>\n",
       "      <td>สู้ๆครับ\\n\\n</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6856 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   word  toxic1  toxic2  \\\n",
       "0     หวั่นระบาดซ้ำ! ฮ่องกง ขยายเวลาเว้นระยะห่างป้อง...     NaN     NaN   \n",
       "1     Suzu Hirose นักแสดงชาวญี่ปุ่นได้รับการยืนยันว่...     NaN     NaN   \n",
       "2      ัดพระธรรมกาย&amp;มูลนิธิธรรมกาย\\nมอบความช่วยเ...     NaN     NaN   \n",
       "3     ตะลึง! องค์การอนามัยโลก (WHO) ชี้ผู้ติดเชื้อ ิ...     NaN     NaN   \n",
       "4     อย่าลืมสวมหน้ากาก รักษาระยะห่าง หมั่นล้างมือ น...     NaN     NaN   \n",
       "...                                                 ...     ...     ...   \n",
       "6851                ช่วยกันค่ะ  ๊อป18ตุลา    ๊อป14ตุลา      NaN     NaN   \n",
       "6852    เกลียดเผด็จการ ีดเส้นใต้ไล่เผด็จการ ็อบ18ตุลา       NaN     NaN   \n",
       "6853  ทำไงดีวะกูอยู่ที่บ้านสลิ่มนั่งฟังจนป่วยจิตหมดล...     NaN     NaN   \n",
       "6854  \"ไม่รักพ่อ ออกจากบ้านพ่อไป\" อ๋อ ออกไม่ได้ค่ะ ช...     NaN     NaN   \n",
       "6855                                   สู้ๆครับ\\n\\n         NaN     NaN   \n",
       "\n",
       "      toxic3  \n",
       "0        NaN  \n",
       "1        NaN  \n",
       "2        NaN  \n",
       "3        NaN  \n",
       "4        NaN  \n",
       "...      ...  \n",
       "6851     NaN  \n",
       "6852     NaN  \n",
       "6853     NaN  \n",
       "6854     NaN  \n",
       "6855     NaN  \n",
       "\n",
       "[6856 rows x 4 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Data = pd.read_csv('dataset_from_scrapper/dataset_tweet.csv')\n",
    "Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # เป็นส่วนเตรียมข้อมูลให้พร้อมสำหรับการ predict\n",
    "# def getinput(train):\n",
    "    \n",
    "#     #เป็นลิสต์สำหรับเก็บคำที่ตัดของแต่ละประโยคที่เข้ามา     \n",
    "#     list_word = []\n",
    "    \n",
    "#     #ทำการตัดประโยคเป็นคำด้วย word_tokenize ของ pythainlp ที่ import เข้ามา     \n",
    "#     proc = word_tokenize(train, engine='newmm')\n",
    "    \n",
    "#     # เก็บคำที่ตัดเอาไว้ที่ลิสต์    \n",
    "#     list_word.append(proc)\n",
    "    \n",
    "#     #สร้างลิสต์สำหรับเก็บลำดับของแต่ละคำภายใน list_word \n",
    "#     sequence = list()\n",
    "    \n",
    "#     for n in list_word: #ลูปเปลี่ยน คำ string ที่ตัดมาแปลงเป็นตัวเลข\n",
    "#         #เป็นตัวเก็บลำดับชั่วคราวจะรีเซ็ตทุกๆรอบ         \n",
    "#         sequenceTemp = list() \n",
    "        \n",
    "#         for w in n:\n",
    "#             count=0\n",
    "#             for x in dict_fileWord['Unnamed: 0']: #ลูปเทียบdictคำ\n",
    "#                 count=count+1\n",
    "#                 if x==w:\n",
    "#                     sequenceTemp.append(count)\n",
    "#                     break\n",
    "#         sequence.append(sequenceTemp)  \n",
    "        \n",
    "#     test = np.array(sequence)\n",
    "\n",
    "#     testX = pad_sequences(test, maxlen=105, padding=\"post\") #input_length=105\n",
    "#     return testX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ตอนนี้ยังไม่ใช้นะ ไปใช้ที่ autolabel ที่เดียว\n",
    "# def normalize_v1(data):\n",
    "#     # กระบวนการหาผลลัพธ์ เตรียมใส label อัตโนมัติ\n",
    "# #     print(data['word'])\n",
    "#     label_toxic1=[]\n",
    "#     label_toxic2=[]\n",
    "#     label_toxic3=[]\n",
    "    \n",
    "#     #รับ dataframe ทั้งอันเข้ามา เข้าถึงคอลัมป์ word\n",
    "#     for x in data['word']:\n",
    "           \n",
    "#         list_label_toxic = [] #สำหรับเก็บค่า index ที่เป็น multilabel\n",
    "        \n",
    "#         # เตรียมอินพุตสำหรับการ predict \n",
    "#         test=getinput(x)\n",
    "#         #print(test)\n",
    "        \n",
    "#         #เรียกโมเดลออกมาใช้ predict\n",
    "#         ynew = clawing_model.predict(test)\n",
    "        \n",
    "#         # สร้างลิสต์ออกมาเพื่อเก็บผลการทำนาย    \n",
    "#         yn = []\n",
    "        \n",
    "#         #วนลูปใน ynew เพื่อใช้งานค่าพยากรณ์ \n",
    "#         for b in ynew[0]:\n",
    "            \n",
    "#             #แปลงเลขให้อยู่ในทศนิยม 4 ตำแหน่ง\n",
    "#             num_per = '%.4f' % b\n",
    "            \n",
    "#             #เก็บเลขที่แปลงเข้าไป\n",
    "#             yn.append(num_per)\n",
    "        \n",
    "#         #  print(ynew[0])\n",
    "#         #หาค่าที่มากที่สุดเพื่อเป็นตัวเทียบ multilabel         \n",
    "#         max_ynew = max(ynew[0][:])\n",
    "# #         print('max :-- ','%.4f' % max_ynew)\n",
    "\n",
    "#         #คิดค่าจาก 10 เปอร์ของตัดมากสุด เพื่อเทียบความใกล้เคียงของคะแนน\n",
    "#         tenPer_max_ynew = max_ynew * 0.9\n",
    "# #         print('tenper --','%.4f' % tenPer_max_ynew)\n",
    "# #         print('result_predict :====',yn)\n",
    "# #                                                                                      passed\n",
    "\n",
    "\n",
    "#         list_label=[] #เก็บค่าที่เป็น Multilabel ไว้ที่นี้\n",
    "#         for i in yn:\n",
    "\n",
    "#             #print('multilabel :++++++++++++',i)\n",
    "#             if(i >= '%.4f' %tenPer_max_ynew ):\n",
    "#                 list_label.append(i)\n",
    "# #         print('ผลลัพธ์จากการทำนาย Multilabel ::::::::::',list_label)                                 #passed \n",
    "\n",
    "#     #     เก็บตำแหน่งที่เป็น multilabel โดยการเทียบกับ yn\n",
    "#         for ii in list_label:\n",
    "            \n",
    "#             count = 0 \n",
    "            \n",
    "#             for iii in yn:\n",
    "                \n",
    "#                 count+=1\n",
    "                \n",
    "#                 if(ii == iii):\n",
    "#                     list_label_toxic.append(count)\n",
    "#                     break\n",
    "# #         print('ตำแหน่งของ Multilabel จากที่ทำนาย ::::::::::',list_label_toxic)                      #passed \n",
    "\n",
    "#         count_j=0\n",
    "#         count_len=0\n",
    "#         for j in list_label_toxic:# มีขนาดตามตำแหน่งที่รับมา\n",
    "#             count_j+=1\n",
    "\n",
    "#             if(count_j==1):\n",
    "# #                 print('Index_multilabel::::::::::',list_label_toxic[0],'lenght :::: 1')\n",
    "#                 label_toxic1.append(list_label_toxic[0])\n",
    "#                 count_len=1\n",
    "\n",
    "#             if(count_j==2):\n",
    "# #                 print('Index_multilabel::::::::::',list_label_toxic[0:2],'lenght :::: 2')\n",
    "#                 label_toxic2.append(list_label_toxic[1])\n",
    "#                 count_len=2\n",
    "\n",
    "#             if(count_j==3):\n",
    "# #                 print('Index_multilabel::::::::::',list_label_toxic[0:3],'lenght :::: 3')\n",
    "#                 label_toxic3.append(list_label_toxic[2])\n",
    "#                 count_len=3\n",
    "#                                                                                     #passed \n",
    "#     # แทนที่ตำแหน่งที่ไม่มีด้วย 0\n",
    "#         if( count_len==1):\n",
    "#             label_toxic2.append(0)\n",
    "#             label_toxic3.append(0)\n",
    "#         if( count_len==2):\n",
    "#             label_toxic3.append(0)\n",
    "#     #    ตรวจตำแหน่งอีกรอบด้วยตาตามแนวตั้ง     \n",
    "# #     print(label_toxic1,label_toxic2,label_toxic3) \n",
    "\n",
    "#     # เตรียมทำ csv แบบที่มีการทำนายผลแล้วเรียบร้อย\n",
    "#     df = pd.DataFrame({'word':nonePredict_documents['word'],\n",
    "#                                     'toxic1': label_toxic1,\n",
    "#                                     'toxic2': label_toxic2,\n",
    "#                                     'toxic3': label_toxic3})\n",
    "\n",
    "#     df.to_csv('new_csv.csv',index=False,encoding='utf-8-sig')\n",
    "# #     label_documents = pd.read_csv ('new_csv.csv')\n",
    "\n",
    "# #     print('label_document : ',label_documents)\n",
    "#     return 'normalize complete'\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ของเก่าเก็บไว้ดูก่อน\n",
    "# def complete_func(input_from_user):\n",
    "    \n",
    "#     # ทำการค้นหาคีย์เวิร์ด    \n",
    "#     tw = query_tweets(input_from_user,limit=None)\n",
    "    \n",
    "#     # export to dataframe and make this to csv แบบสามแถว\n",
    "#     to_csv_v2(tw)\n",
    "    \n",
    "#     global clawing_model\n",
    "#     clawing_model = load_model('cnn&rnn_model.h5')\n",
    "\n",
    "#     global dict_fileWord\n",
    "#     dict_fileWord=pd.read_csv('dict_file.csv')\n",
    "    \n",
    "#     global test_documents\n",
    "#     test_documents = pd.read_csv ('dataset_tweet.csv')\n",
    "#     # test_documents\n",
    "    \n",
    "#     normalize_v1(test_documents)\n",
    "    \n",
    "#     complete_file = pd.read_csv ('./new_csv.csv')\n",
    "#     # complete_file\n",
    "#     print(complete_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# อันเก่าเก็บไว้ดูก่อน\n",
    "# # แบบสามแถว\n",
    "# def to_csv_v2(tw):    \n",
    "#     # read previous csv file before append the new word\n",
    "#         old_csv = pd.read_csv('dataset_tweet.csv');\n",
    "#         if(len(old_csv ) !=0 ):\n",
    "#             list_tw = []\n",
    "#             for word in old_csv['word']:\n",
    "#                 list_tw.append(word)\n",
    "#             print('get old list_tw:',list_tw)\n",
    "#         else:\n",
    "#             # make new list to save text from  tweet object\n",
    "#            # only first time to create csv file    \n",
    "#             list_tw = []\n",
    "#         # loop on list of tweet object\n",
    "#         for t in tw:\n",
    "#             list_tw.append(t.text);\n",
    "#         print('list_tw:',list_tw)\n",
    "#         df = pd.DataFrame({'word':list_tw,\n",
    "#                             'toxic1':None,\n",
    "#                            'toxic2':None,\n",
    "#                            'toxic3':None})\n",
    "\n",
    "#         df.to_csv('dataset_tweet.csv',index=False,encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
